LLAMAFILE(1)                General Commands Manual               LLAMAFILE(1)

NNAAMMEE
     llllaammaaffiillee - large language model runner

SSYYNNOOPPSSIISS
     llllaammaaffiillee [----sseerrvveerr] [flags...] --mm _m_o_d_e_l_._g_g_u_f [----mmmmpprroojj _v_i_s_i_o_n_._g_g_u_f]
     llllaammaaffiillee [----ccllii] [flags...] --mm _m_o_d_e_l_._g_g_u_f --pp _p_r_o_m_p_t
     llllaammaaffiillee [----ccllii] [flags...]
               --mm _m_o_d_e_l_._g_g_u_f ----mmmmpprroojj _v_i_s_i_o_n_._g_g_u_f ----iimmaaggee _g_r_a_p_h_i_c_._p_n_g
               --pp _p_r_o_m_p_t

DDEESSCCRRIIPPTTIIOONN
     llllaammaaffiillee is a large language model tool. It has use cases such as:

     --   Code completion
     --   Prose composition
     --   Chatbot that passes the Turing test
     --   Text/image summarization and analysis

OOPPTTIIOONNSS
     The following options are available:

     ----vveerrssiioonn
             Print version and exit.

     --hh, ----hheellpp
             Show help message and exit.

     ----ccllii   Puts program in command line interface mode. This flag is implied
             when a prompt is supplied using either the --pp or --ff flags.

     ----sseerrvveerr
             Puts program in server mode. This will launch an HTTP server on a
             local port. This server has both a web UI and an OpenAI API
             compatible completions endpoint. When the server is run on a desk
             system, a tab browser tab will be launched automatically that
             displays the web UI.  This ----sseerrvveerr flag is implied if no prompt
             is specified, i.e. neither the --pp or --ff flags are passed.

     --mm _F_N_A_M_E, ----mmooddeell _F_N_A_M_E
             Model path in the GGUF file format.

             Default: _m_o_d_e_l_s_/_7_B_/_g_g_m_l_-_m_o_d_e_l_-_f_1_6_._g_g_u_f

     ----mmmmpprroojj _F_N_A_M_E
             Specifies path of the LLaVA vision model in the GGUF file format.
             If this flag is supplied, then the ----mmooddeell and ----iimmaaggee flags
             should also be supplied.

     --ss _S_E_E_D, ----sseeeedd _S_E_E_D
             Random Number Generator (RNG) seed. A random seed is used if this
             is less than zero.

             Default: -1

     --tt _N, ----tthhrreeaaddss _N
             Number of threads to use during generation.

             Default: $(nproc)/2

     --ttbb _N, ----tthhrreeaaddss--bbaattcchh _N
             Number of threads to use during batch and prompt processing.

             Default: Same as ----tthhrreeaaddss

     ----iinn--pprreeffiixx--bbooss
             Prefix BOS to user inputs, preceding the ----iinn--pprreeffiixx string.

     ----iinn--pprreeffiixx _S_T_R_I_N_G
             String to prefix user inputs with.

             Default: empty

     ----iinn--ssuuffffiixx _S_T_R_I_N_G
             String to suffix after user inputs with.

             Default: empty

     --nn _N, ----nn--pprreeddiicctt _N
             Number of tokens to predict.

             --   -1 = infinity
             --   -2 = until context filled

             Default: -1

     --cc _N, ----ccttxx--ssiizzee _N
             Size of the prompt context.

             --   0 = loaded from model

             Default: 512

     --bb _N, ----bbaattcchh--ssiizzee _N
             Batch size for prompt processing.

             Default: 512

     ----ttoopp--kk _N
             Top-k sampling.

             --   0 = disabled

             Default: 40

     ----ttoopp--pp _N
             Top-p sampling.

             --   1.0 = disabled

             Default: 0.9

     ----mmiinn--pp _N
             Min-p sampling.

             --   0.0 = disabled

             Default: 0.1

     ----ttffss _N
             Tail free sampling, parameter z.

             --   1.0 = disabled

             Default: 1.0

     ----ttyyppiiccaall _N
             Locally typical sampling, parameter p.

             --   1.0 = disabled

             Default: 1.0

     ----rreeppeeaatt--llaasstt--nn _N
             Last n tokens to consider for penalize.

             --   0 = disabled
             --   -1 = ctx_size

             Default: 64

     ----rreeppeeaatt--ppeennaallttyy _N
             Penalize repeat sequence of tokens.

             --   1.0 = disabled

             Default: 1.1

     ----pprreesseennccee--ppeennaallttyy _N
             Repeat alpha presence penalty.

             --   0.0 = disabled

             Default: 0.0

     ----ffrreeqquueennccyy--ppeennaallttyy _N
             Repeat alpha frequency penalty.

             --   0.0 = disabled

             Default: 0.0

     ----mmiirroossttaatt _N
             Use Mirostat sampling. Top K, Nucleus, Tail Free and Locally
             Typical samplers are ignored if used..

             --   0 = disabled
             --   1 = Mirostat
             --   2 = Mirostat 2.0

             Default: 0

     ----mmiirroossttaatt--llrr _N
             Mirostat learning rate, parameter eta.

             Default: 0.1

     ----mmiirroossttaatt--eenntt _N
             Mirostat target entropy, parameter tau.

             Default: 5.0

     --ll _T_O_K_E_N___I_D_(_+_/_-_)_B_I_A_S, ----llooggiitt--bbiiaass _T_O_K_E_N___I_D_(_+_/_-_)_B_I_A_S
             Modifies the likelihood of token appearing in the completion,
             i.e.  ----llooggiitt--bbiiaass _1_5_0_4_3_+_1 to increase likelihood of token
             _' _H_e_l_l_o_', or ----llooggiitt--bbiiaass _1_5_0_4_3_-_1 to decrease likelihood of token
             _' _H_e_l_l_o_'.

     --mmdd _F_N_A_M_E, ----mmooddeell--ddrraafftt _F_N_A_M_E
             Draft model for speculative decoding.

             Default: _m_o_d_e_l_s_/_7_B_/_g_g_m_l_-_m_o_d_e_l_-_f_1_6_._g_g_u_f

     ----ccffgg--nneeggaattiivvee--pprroommpptt _P_R_O_M_P_T
             Negative prompt to use for guidance..

             Default: empty

     ----ccffgg--nneeggaattiivvee--pprroommpptt--ffiillee _F_N_A_M_E
             Negative prompt file to use for guidance.

             Default: empty

     ----ccffgg--ssccaallee _N
             Strength of guidance.

             --   1.0 = disable

             Default: 1.0

     ----rrooppee--ssccaalliinngg _{_n_o_n_e_,_l_i_n_e_a_r_,_y_a_r_n_}
             RoPE frequency scaling method, defaults to linear unless
             specified by the model

     ----rrooppee--ssccaallee _N
             RoPE context scaling factor, expands context by a factor of N

     ----rrooppee--ffrreeqq--bbaassee _N
             RoPE base frequency, used by NTK-aware scaling.

             Default: loaded from model

     ----rrooppee--ffrreeqq--ssccaallee _N
             RoPE frequency scaling factor, expands context by a factor of 1/N

     ----yyaarrnn--oorriigg--ccttxx _N
             YaRN: original context size of model.

             Default: 0 = model training context size

     ----yyaarrnn--eexxtt--ffaaccttoorr _N
             YaRN: extrapolation mix factor.

             --   0.0 = full interpolation

             Default: 1.0

     ----yyaarrnn--aattttnn--ffaaccttoorr _N
             YaRN: scale sqrt(t) or attention magnitude.

             Default: 1.0

     ----yyaarrnn--bbeettaa--ssllooww _N
             YaRN: high correction dim or alpha.

             Default: 1.0

     ----yyaarrnn--bbeettaa--ffaasstt _N
             YaRN: low correction dim or beta.

             Default: 32.0

     ----iiggnnoorree--eeooss
             Ignore end of stream token and continue generating (implies
             ----llooggiitt--bbiiaass _2_-_i_n_f)

     ----nnoo--ppeennaalliizzee--nnll
             Do not penalize newline token.

     ----tteemmpp _N
             Temperature.

             Default: 0.8

     ----llooggiittss--aallll
             Return logits for all tokens in the batch.

             Default: disabled

     ----hheellllaasswwaagg
             Compute HellaSwag score over random tasks from datafile supplied
             with -f

     ----hheellllaasswwaagg--ttaasskkss _N
             Number of tasks to use when computing the HellaSwag score.

             Default: 400

     ----kkeeeepp _N
             Number of tokens to keep from the initial prompt.

             --   -1 = all

             Default: 0

     ----ddrraafftt _N
             Number of tokens to draft for speculative decoding.

             Default: 16

     ----cchhuunnkkss _N
             Max number of chunks to process.

             --   -1 = all

             Default: -1

     --nnss _N, ----sseeqquueenncceess _N
             Number of sequences to decode.

             Default: 1

     --ppaa _N, ----pp--aacccceepptt _N
             speculative decoding accept probability.

             Default: 0.5

     --ppss _N, ----pp--sspplliitt _N
             Speculative decoding split probability.

             Default: 0.1

     ----mmlloocckk
             Force system to keep model in RAM rather than swapping or
             compressing.

     ----nnoo--mmmmaapp
             Do not memory-map model (slower load but may reduce pageouts if
             not using mlock).

     ----nnuummaa  Attempt optimizations that help on some NUMA systems if run
             without this previously, it is recommended to drop the system
             page cache before using this. See
             https://github.com/ggerganov/llama.cpp/issues/1437.

     ----rreeccoommppiillee
             Force GPU support to be recompiled at runtime if possible.

     ----nnooccoommppiillee
             Never compile GPU support at runtime.

             If the appropriate DSO file already exists under _~_/_._l_l_a_m_a_f_i_l_e_/
             then it'll be linked as-is without question. If a prebuilt DSO is
             present in the PKZIP content of the executable, then it'll be
             extracted and linked if possible. Otherwise, llllaammaaffiillee will skip
             any attempt to compile GPU support and simply fall back to using
             CPU inference.

     ----ggppuu _G_P_U
             Specifies which brand of GPU should be used. Valid choices are:

             --   _A_U_T_O: Use any GPU if possible, otherwise fall back to CPU
                 inference (default)

             --   _A_P_P_L_E: Use Apple Metal GPU. This is only available on MacOS
                 ARM64. If Metal could not be used for any reason, then a
                 fatal error will be raised.

             --   _A_M_D: Use AMD GPUs. The AMD HIP ROCm SDK should be installed
                 in which case we assume the HIP_PATH environment variable has
                 been defined. The set of gfx microarchitectures needed to run
                 on the host machine is determined automatically based on the
                 output of the hipInfo command. On Windows, llllaammaaffiillee release
                 binaries are distributed with a tinyBLAS DLL so it'll work
                 out of the box without requiring the HIP SDK to be installed.
                 However, tinyBLAS is slower than rocBLAS for batch and image
                 processing, so it's recommended that the SDK be installed
                 anyway. If an AMD GPU could not be used for any reason, then
                 a fatal error will be raised.

             --   _N_V_I_D_I_A: Use NVIDIA GPUs. If an NVIDIA GPU could not be used
                 for any reason, a fatal error will be raised. On Windows,
                 NVIDIA GPU support will use our tinyBLAS library, since it
                 works on stock Windows installs. However, tinyBLAS goes
                 slower for batch and image processing. It's possible to use
                 NVIDIA's closed-source cuBLAS library instead. To do that,
                 both MSVC and CUDA need to be installed and the llllaammaaffiillee
                 command should be run once from the x64 MSVC command prompt
                 with the ----rreeccoommppiillee flag passed. The GGML library will then
                 be compiled and saved to _~_/_._l_l_a_m_a_f_i_l_e_/ so the special process
                 only needs to happen a single time.

             --   _D_I_S_A_B_L_E: Never use GPU and instead use CPU inference. This
                 setting is implied by --nnggll _0.

     --nnggll _N, ----nn--ggppuu--llaayyeerrss _N
             Number of layers to store in VRAM.

     --nngglldd _N, ----nn--ggppuu--llaayyeerrss--ddrraafftt _N
             Number of layers to store in VRAM for the draft model.

     --ttss _S_P_L_I_T, ----tteennssoorr--sspplliitt _S_P_L_I_T
             How to split tensors across multiple GPUs, comma-separated list
             of proportions, e.g. 3,1

     --mmgg _i, ----mmaaiinn--ggppuu _i
             The GPU to use for scratch and small tensors.

     --nnoommmmqq, ----nnoo--mmuull--mmaatt--qq
             Use cuBLAS instead of custom mul_mat_q CUDA kernels. Not
             recommended since this is both slower and uses more VRAM.

     ----vveerrbboossee--pprroommpptt
             Print prompt before generation.

     ----ssiimmppllee--iioo
             Use basic IO for better compatibility in subprocesses and limited
             consoles.

     ----lloorraa _F_N_A_M_E
             Apply LoRA adapter (implies ----nnoo--mmmmaapp)

     ----lloorraa--ssccaalleedd _F_N_A_M_E _S
             Apply LoRA adapter with user defined scaling S (implies
             ----nnoo--mmmmaapp)

     ----lloorraa--bbaassee _F_N_A_M_E
             Optional model to use as a base for the layers modified by the
             LoRA adapter

     ----uunnsseeccuurree
             Disables pledge() sandboxing on Linux and OpenBSD.

     ----ssaammpplleerrss
             Samplers that will be used for generation in the order, separated
             by semicolon, for example: top_k;tfs;typical;top_p;min_p;temp

     ----ssaammpplleerrss--sseeqq
             Simplified sequence for samplers that will be used.

     --ccmmll, ----cchhaattmmll
             Run in chatml mode (use with ChatML-compatible models)

     --ddkkvvcc, ----dduummpp--kkvv--ccaacchhee
             Verbose print of the KV cache.

     --nnkkvvoo, ----nnoo--kkvv--ooffffllooaadd
             Disable KV offload.

     --ccttkk _T_Y_P_E, ----ccaacchhee--ttyyppee--kk _T_Y_P_E
             KV cache data type for K.

     --ccttvv _T_Y_P_E, ----ccaacchhee--ttyyppee--vv _T_Y_P_E
             KV cache data type for V.

CCLLII OOPPTTIIOONNSS
     The following options may be specified when llllaammaaffiillee is running in ----ccllii
     mode.

     --ee, ----eessccaappee
             Process prompt escapes sequences (\n, \r, \t, \', \", \\)

     --pp _S_T_R_I_N_G, ----pprroommpptt _S_T_R_I_N_G
             Prompt to start text generation. Your LLM works by auto-
             completing this text. For example:

                   llamafile -m model.gguf -p "four score and"

             Stands a pretty good chance of printing Lincoln's Gettysburg
             Address.  Prompts can take on a structured format too. Depending
             on how your model was trained, it may specify in its docs an
             instruction notation. With some models that might be:

                   llamafile -p "[INST]Summarize this: $(cat file)[/INST]"

             In most cases, simply colons and newlines will work too:

                   llamafile -e -p "User: What is best in life?\nAssistant:"

     --ff _F_N_A_M_E, ----ffiillee _F_N_A_M_E
             Prompt file to start generation.

     ----ggrraammmmaarr _G_R_A_M_M_A_R
             BNF-like grammar to constrain which tokens may be selected when
             generating text. For example, the grammar:

                   root ::= "yes" | "no"

             will force the LLM to only output yes or no before exiting. This
             is useful for shell scripts when the ----ssiilleenntt--pprroommpptt flag is also
             supplied.

     ----ggrraammmmaarr--ffiillee _F_N_A_M_E
             File to read grammar from.

     ----pprroommpptt--ccaacchhee _F_N_A_M_E
             File to cache prompt state for faster startup.

             Default: none

     ----pprroommpptt--ccaacchhee--aallll
             If specified, saves user input and generations to cache as well.
             Not supported with ----iinntteerraaccttiivvee or other interactive options.

     ----pprroommpptt--ccaacchhee--rroo
             If specified, uses the prompt cache but does not update it.

     ----rraannddoomm--pprroommpptt
             Start with a randomized prompt.

     ----iimmaaggee _I_M_A_G_E___F_I_L_E
             Path to an image file. This should be used with multimodal
             models.  Alternatively, it's possible to embed an image directly
             into the prompt instead; in which case, it must be base64 encoded
             into an HTML img tag URL with the image/jpeg MIME type. See also
             the ----mmmmpprroojj flag for supplying the vision model.

     --ii, ----iinntteerraaccttiivvee
             Run in interactive mode.

     ----iinntteerraaccttiivvee--ffiirrsstt
             Run in interactive mode and wait for input right away.

     --iinnss, ----iinnssttrruucctt
             Run in instruction mode (use with Alpaca models).

     --rr _P_R_O_M_P_T, ----rreevveerrssee--pprroommpptt _P_R_O_M_P_T
             Halt generation at _P_R_O_M_P_T and return control in interactive mode
             (can be specified more than once for multiple prompts).

     ----ccoolloorr
             Colorise output to distinguish prompt and user input from
             generations.

     ----ssiilleenntt--pprroommpptt
             Don't echo the prompt itself to standard output.

     ----mmuullttiilliinnee--iinnppuutt
             Allows you to write or paste multiple lines without ending each
             in '\'.

SSEERRVVEERR OOPPTTIIOONNSS
     The following options may be specified when llllaammaaffiillee is running in
     ----sseerrvveerr mode.

     ----ppoorrtt _P_O_R_T
             Port to listen

             Default: 8080

     ----hhoosstt _I_P_A_D_D_R
             IP address to listen.

             Default: 127.0.0.1

     --ttoo _N, ----ttiimmeeoouutt _N
             Server read/write timeout in seconds.

             Default: 600

     --nnpp _N, ----ppaarraalllleell _N
             Number of slots for process requests.

             Default: 1

     --ccbb, ----ccoonntt--bbaattcchhiinngg
             Enable continuous batching (a.k.a dynamic batching).

             Default: disabled

     --ssppff _F_N_A_M_E, ----ssyysstteemm--pprroommpptt--ffiillee _F_N_A_M_E
             Set a file to load a system prompt (initial prompt of all slots),
             this is useful for chat applications.

     --aa _A_L_I_A_S, ----aalliiaass _A_L_I_A_S
             Set an alias for the model. This will be added as the _m_o_d_e_l field
             in completion responses.

     ----ppaatthh _P_U_B_L_I_C___P_A_T_H
             Path from which to serve static files.

             Default: _/_z_i_p_/_l_l_a_m_a_._c_p_p_/_s_e_r_v_e_r_/_p_u_b_l_i_c

     ----eemmbbeeddddiinngg
             Enable embedding vector output.

             Default: disabled

     ----nnoobbrroowwsseerr
             Do not attempt to open a web browser tab at startup.

LLOOGG OOPPTTIIOONNSS
     The following log options are available:

     --lldd _L_O_G_D_I_R, ----llooggddiirr _L_O_G_D_I_R
             Path under which to save YAML logs (no logging if unset)

     ----lloogg--tteesstt
             Run simple logging test

     ----lloogg--ddiissaabbllee
             Disable trace logs

     ----lloogg--eennaabbllee
             Enable trace logs

     ----lloogg--ffiillee
             Specify a log filename (without extension)

     ----lloogg--nneeww
             Create a separate new log file on start. Each log file will have
             unique name: _<_n_a_m_e_>_._<_I_D_>_._l_o_g

     ----lloogg--aappppeenndd
             Don't truncate the old log file.

EEXXAAMMPPLLEESS
     Here's an example of how to run llama.cpp's built-in HTTP server. This
     example uses LLaVA v1.5-7B, a multimodal LLM that works with llama.cpp's
     recently-added support for image inputs.

           llamafile \
             -m llava-v1.5-7b-Q8_0.gguf \
             --mmproj llava-v1.5-7b-mmproj-Q8_0.gguf \
             --host 0.0.0.0

     Here's an example of how to generate code for a libc function using the
     llama.cpp command line interface, utilizing WizardCoder-Python-13B
     weights:

           llamafile \
             -m wizardcoder-python-13b-v1.0.Q8_0.gguf --temp 0 -r '}\n' -r '```\n' \
             -e -p '```c\nvoid *memcpy(void *dst, const void *src, size_t size) {\n'

     Here's a similar example that instead utilizes Mistral-7B-Instruct
     weights for prose composition:

           llamafile \
             -m mistral-7b-instruct-v0.2.Q5_K_M.gguf \
             -p '[INST]Write a story about llamas[/INST]'

     Here's an example of how llamafile can be used as an interactive chatbot
     that lets you query knowledge contained in training data:

           llamafile -m llama-65b-Q5_K.gguf -p '
           The following is a conversation between a Researcher and their helpful AI
           assistant Digital Athena which is a large language model trained on the
           sum of human knowledge.
           Researcher: Good morning.
           Digital Athena: How can I help you today?
           Researcher:' --interactive --color --batch_size 1024 --ctx_size 4096 \
           --keep -1 --temp 0 --mirostat 2 --in-prefix ' ' --interactive-first \
           --in-suffix 'Digital Athena:' --reverse-prompt 'Researcher:'

     Here's an example of how you can use llamafile to summarize HTML URLs:

           (
             echo '[INST]Summarize the following text:'
             links -codepage utf-8 \
                   -force-html \
                   -width 500 \
                   -dump https://www.poetryfoundation.org/poems/48860/the-raven |
               sed 's/   */ /g'
             echo '[/INST]'
           ) | llamafile \
                 -m mistral-7b-instruct-v0.2.Q5_K_M.gguf \
                 -f /dev/stdin \
                 -c 0 \
                 --temp 0 \
                 -n 500 \
                 --silent-prompt 2>/dev/null

     Here's how you can use llamafile to describe a jpg/png/gif/bmp image:

           llamafile --temp 0 \
             --image lemurs.jpg \
             -m llava-v1.5-7b-Q4_K.gguf \
             --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \
             -e -p '### User: What do you see?\n### Assistant: ' \
             --silent-prompt 2>/dev/null

     If you wanted to write a script to rename all your image files, you could
     use the following command to generate a safe filename:

           llamafile --temp 0 \
               --image ~/Pictures/lemurs.jpg \
               -m llava-v1.5-7b-Q4_K.gguf \
               --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \
               --grammar 'root ::= [a-z]+ (" " [a-z]+)+' \
               -e -p '### User: The image has...\n### Assistant: ' \
               --silent-prompt 2>/dev/null |
             sed -e's/ /_/g' -e's/$/.jpg/'
           three_baby_lemurs_on_the_back_of_an_adult_lemur.jpg

     Here's an example of how to make an API request to the OpenAI API
     compatible completions endpoint when your llllaammaaffiillee is running in the
     background in ----sseerrvveerr mode.

           curl -s http://localhost:8080/v1/chat/completions \
                -H "Content-Type: application/json" -d '{
             "model": "gpt-3.5-turbo",
             "stream": true,
             "messages": [
               {
                 "role": "system",
                 "content": "You are a poetic assistant."
               },
               {
                 "role": "user",
                 "content": "Compose a poem that explains FORTRAN."
               }
             ]
           }' | python3 -c '
           import json
           import sys
           json.dump(json.load(sys.stdin), sys.stdout, indent=2)
           print()

BBUUGGSS
     Your HTTP server is great for local hosting, but it would need much more
     work to be a production worthy component of a public-facing service. For
     example, C++ exceptions caused by JSON parsing errors will make it abort
     and print a backtrace.

PPRROOTTIIPP
     The --nnggll _3_5 flag needs to be passed in order to use GPUs made by NVIDIA
     and AMD.  It's not enabled by default since it sometimes needs to be
     tuned based on the system hardware and model architecture, in order to
     achieve optimal performance, and avoid compromising a shared display.

SSEEEE AALLSSOO
     llamafile-quantize(1), llamafile-perplexity(1), llava-quantize(1),
     zipalign(1), unzip(1)

Mozilla Ocho                    January 1, 2024                   Mozilla Ocho
