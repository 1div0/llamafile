DESCRIPTION

  llama.cpp is a machine learning library for large language models

LICENSE

  MIT

ORIGIN

  https://github.com/ggerganov/llama.cpp/pull/4406/
  973053d8b0d04809836b3339a50f68d9c842de90
  2024-02-22

LOCAL MODIFICATIONS

  - Refactor ggml.c, llama.cpp, and llava to use llamafile_open() APIs
  - Unify main, server, and llava-cli into single llamafile program
  - Make cuBLAS / hipBLAS optional by introducing tinyBLAS library
  - Add support to main() programs for Cosmo /zip/.args files
  - Introduce pledge() SECCOMP sandboxing to improve security
  - Call exit() rather than abort() when GGML_ASSERT() fails
  - Make GPU logger callback API safer and less generic
  - Write log to /dev/null when main.log fails to open
  - Use _rand64() rather than time() as default seed
  - Make main and llava-cli print timings on ctrl-c
  - Avoid bind() conflicts on port 8080 w/ server
  - Use runtime dispatching for matmul quants
  - Remove operating system #ifdef statements
  - Remove stdout logging from LLaVA
