DESCRIPTION

  llama.cpp is a machine learning library for large language models

LICENSE

  MIT

ORIGIN

  Sources were initially imported from:

    https://github.com/ggerganov/llama.cpp
    e9c1cecb9d7d743d30b4a29ecd56a411437def0a
    Tue Nov 7 09:04:51 2023 +0100

  Since then, we've cherry-picked a number of small changes which fix
  bugs and add features. The changes we haven't cherry-picked yet are
  Georgi and Slaren's active development work, improving tensor graph
  allocation, scheduling, backends, etc. So far, we've curated up to:

    https://github.com/ggerganov/llama.cpp
    8d6d9f033b8101f929e445cf45b39e1557ca7934
    Fri Dec 1 11:41:56 2023 +0200

  See the llamafile git history re: cherry-picks for further details.

LOCAL MODIFICATIONS

  - Refactor ggml.c, llama.cpp, and llava to use llamafile_open() APIs
  - Add support to main() programs for Cosmo /zip/.args files
  - Use condition variables for server rather than busy loop
  - Introduce pledge() SECCOMP sandboxing to improve security
  - Remove log callback pointer API from Metal GPU module
  - Use runtime dispatching for matmul quants
  - Remove operating system #ifdef statements
  - Introduce --silent-prompt flag to main
