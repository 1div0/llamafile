DESCRIPTION

  llama.cpp is a machine learning library for large language models

LICENSE

  MIT

ORIGIN

  https://github.com/ggerganov/llama.cpp/pull/4406/
  67fac4b95fcccfda8ab965e9ba4992a9ddf3a25f
  2024-04-10

LOCAL MODIFICATIONS

  - Count the number of cores correctly on Intel's Alderlake architecture
  - Remove MAP_POPULATE because it makes mmap(tinyllama) block for 100ms
  - Refactor ggml.c, llama.cpp, and llava to use llamafile_open() APIs
  - Unify main, server, and llava-cli into single llamafile program
  - Make cuBLAS / hipBLAS optional by introducing tinyBLAS library
  - Add support to main() programs for Cosmo /zip/.args files
  - Introduce pledge() SECCOMP sandboxing to improve security
  - Call exit() rather than abort() when GGML_ASSERT() fails
  - Make GPU logger callback API safer and less generic
  - Write log to /dev/null when main.log fails to open
  - Use _rand64() rather than time() as default seed
  - Make main and llava-cli print timings on ctrl-c
  - Avoid bind() conflicts on port 8080 w/ server
  - Use runtime dispatching for matmul quants
  - Remove operating system #ifdef statements
  - Remove stdout logging from LLaVA
