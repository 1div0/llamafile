LLAMAFILE-QUANTIZE(1)     BSD General Commands Manual    LLAMAFILE-QUANTIZE(1)

NNAAMMEE
     llllaammaaffiillee--qquuaannttiizzee — large language model quantizer

SSYYNNOOPPSSIISS
     llllaammaaffiillee--qquuaannttiizzee [flags...] _m_o_d_e_l_-_f_3_2_._g_g_u_f [_m_o_d_e_l_-_q_u_a_n_t_._g_g_u_f] _t_y_p_e
                        [_n_t_h_r_e_a_d_s]

DDEESSCCRRIIPPTTIIOONN
     llllaammaaffiillee--qquuaannttiizzee converts large language model weights from the float32
     or float16 formats into smaller data types from 2 to 8 bits in size.

OOPPTTIIOONNSS
     The following flags are available:

     ----aallllooww--rreeqquuaannttiizzee
             Allows requantizing tensors that have already been quantized.
             Warning: This can severely reduce quality compared to quantizing
             from 16bit or 32bit

     ----lleeaavvee--oouuttppuutt--tteennssoorr
             Will leave output.weight un(re)quantized. Increases model size
             but may also increase quality, especially when requantizing

     ----ppuurree  Disable k-quant mixtures and quantize all tensors to the same
             type

AARRGGUUMMEENNTTSS
     The following positional arguments are accepted:

     _m_o_d_e_l_-_f_3_2_._g_g_u_f
             Is the input file, which contains the unquantized model weights
             in either the float32 or float16 format.

     _m_o_d_e_l_-_q_u_a_n_t_._g_g_u_f
             Is the output file, which will contain quantized weights in the
             desired format. If this path isn't specified, it'll default to
             [inp path]/ggml-model-[ftype].gguf.

     _t_y_p_e    Is the desired quantization format, which may be the integer id
             of a supported quantization type, or its name. See the quantiza‐
             tion types section below for acceptable formats.

     _n_t_h_r_e_a_d_s
             Number of threads to use during computation (default: nproc/2)

QQUUAANNTTIIZZAATTIIOONN TTYYPPEESS
     The following quantization types are available:

     --      2 Q4_0   3.56G +0.2166 ppl @ LLaMA-v1-7B
     --      3 Q4_1   3.90G +0.1585 ppl @ LLaMA-v1-7B
     --      8 Q5_0   4.33G +0.0683 ppl @ LLaMA-v1-7B
     --      9 Q5_1   4.70G +0.0349 ppl @ LLaMA-v1-7B
     --     10 Q2_K   2.63G +0.6717 ppl @ LLaMA-v1-7B
     --     12 Q3_K   alias for Q3_K_M
     --     11 Q3_K_S 2.75G +0.5551 ppl @ LLaMA-v1-7B
     --     12 Q3_K_M 3.07G +0.2496 ppl @ LLaMA-v1-7B
     --     13 Q3_K_L 3.35G +0.1764 ppl @ LLaMA-v1-7B
     --     15 Q4_K   alias for Q4_K_M
     --     14 Q4_K_S 3.59G +0.0992 ppl @ LLaMA-v1-7B
     --     15 Q4_K_M 3.80G +0.0532 ppl @ LLaMA-v1-7B
     --     17 Q5_K   alias for Q5_K_M
     --     16 Q5_K_S 4.33G +0.0400 ppl @ LLaMA-v1-7B
     --     17 Q5_K_M 4.45G +0.0122 ppl @ LLaMA-v1-7B
     --     18 Q6_K   5.15G -0.0008 ppl @ LLaMA-v1-7B
     --      7 Q8_0   6.70G +0.0004 ppl @ LLaMA-v1-7B
     --      1 F16    13.00G @ 7B
     --      0 F32    26.00G @ 7B
     --   COPY Only copy tensors, no quantizing.

SSEEEE AALLSSOO
     llamafile(1), llamafile-imatrix(1), llamafile-perplexity(1),
     llava-quantize(1), zipalign(1), unzip(1)

Llamafile Manual               December 5, 2023               Llamafile Manual
