.Dd December 5, 2023
.Dt llamafile-server 1
.Os
.Sh NAME
.Nm llamafile-server
.Nd large language model server
.Sh SYNOPSIS
.Nm llamafile
.Fl Fl server
.Op flags...
.Op Fl m Ar weights.gguf
.Sh DESCRIPTION
.Nm
launches an HTTP server and browser tab with a web user interface for
the llama.cpp software. The UI lets you chat with your LLM and upload
images it can analyze. Furthermore, this server provides an OpenAI API
compatible completions endpoint.
.Pp
.Nm llamafile-server
is launched via the
.Nm llamafile
command when the
.Fl Fl server
flag is passed. The server flag is implied by default if no prompt is
specified, i.e. neither the
.Fl p
or
.Fl f
flags are passed.
.Sh OPTIONS
The following options are available:
.Bl -tag -width indent
.It Fl h , Fl Fl help
Show this help message and exit.
.It Fl v , Fl Fl verbose
Verbose output (default: disabled)
.It Fl t Ar N , Fl Fl threads Ar N
Number of threads to use during computation (default: nproc/2)
.It Fl tb Ar N , Fl Fl threads-batch Ar N
Number of threads to use during batch and prompt processing (default:
same as
.Fl Fl threads )
.It Fl c Ar N , Fl Fl ctx-size Ar N
Size of the prompt context (default: 512, 0 = loaded from model)
.It Fl Fl rope-scaling Ar {none,linear,yarn}
RoPE frequency scaling method, defaults to linear unless specified by the model
.It Fl Fl rope-scale Ar N
RoPE context scaling factor, expands context by a factor of N
.It Fl Fl rope-freq-base Ar N
RoPE base frequency, used by NTK-aware scaling (default: loaded from model)
.It Fl Fl rope-freq-scale Ar N
RoPE frequency scaling factor, expands context by a factor of 1/N
.It Fl Fl yarn-orig-ctx Ar N
YaRN: original context size of model (default: 0 = model training context size)
.It Fl Fl yarn-ext-factor Ar N
YaRN: extrapolation mix factor (default: 1.0, 0.0 = full interpolation)
.It Fl Fl yarn-attn-factor Ar N
YaRN: scale sqrt(t) or attention magnitude (default: 1.0)
.It Fl Fl yarn-beta-slow Ar N
YaRN: high correction dim or alpha (default: 1.0)
.It Fl Fl yarn-beta-fast Ar N
YaRN: low correction dim or beta (default: 32.0)
.It Fl b Ar N , Fl Fl batch-size Ar N
Batch size for prompt processing (default: 512)
.It Fl Fl memory-f32
Use f32 instead of f16 for memory key+value (default: disabled) Not recommended: doubles context memory required and no measurable increase in quality.
.It Fl Fl mlock
Force system to keep model in RAM rather than swapping or compressing.
.It Fl Fl no-mmap
Do not memory-map model (slower load but may reduce pageouts if not using mlock).
.It Fl Fl numa
Attempt optimizations that help on some NUMA systems if run without this previously, it is recommended to drop the system page cache before using this. See https://github.com/ggerganov/llama.cpp/issues/1437.
.It Fl ngl Ar N , Fl Fl n-gpu-layers Ar N
Number of layers to store in VRAM.
.It Fl ts Ar SPLIT , Fl Fl tensor-split Ar SPLIT
How to split tensors across multiple GPUs, comma-separated list of proportions, e.g. 3,1
.It Fl mg Ar i , Fl Fl main-gpu Ar i
The GPU to use for scratch and small tensors.
.It Fl nommq , Fl Fl no-mul-mat-q
Use cuBLAS instead of custom mul_mat_q CUDA kernels. Not recommended since this is both slower and uses more VRAM.
.It Fl m Ar FNAME , Fl Fl model Ar FNAME
Model path (default: models/7B/ggml-model-f16.gguf)
.It Fl a Ar ALIAS , Fl Fl alias Ar ALIAS
Set an alias for the model, will be added as `model` field in completion response.
.It Fl Fl lora Ar FNAME
Apply LoRA adapter (implies
.Fl Fl no-mmap )
.It Fl Fl lora-base Ar FNAME
Optional model to use as a base for the layers modified by the LoRA adapter
.It Fl Fl host Ar IPADDR
IP address to listen (default: 127.0.0.1)
.It Fl Fl port Ar PORT
Port to listen (default  (default: 8080)
.It Fl Fl path Ar PUBLIC_PATH
Path from which to serve static files (default /zip/llama.cpp/server/public)
.It Fl to Ar N , Fl Fl timeout Ar N
Server read/write timeout in seconds (default: 600)
.It Fl Fl embedding
Enable embedding vector output (default: disabled)
.It Fl np Ar N , Fl Fl parallel Ar N
Number of slots for process requests (default: 1)
.It Fl cb , Fl Fl cont-batching
Enable continuous batching (a.k.a dynamic batching) (default: disabled)
.It Fl spf Ar FNAME , Fl Fl system-prompt-file Ar FNAME
Set a file to load a system prompt (initial prompt of all slots), this is useful for chat applications.
.It Fl Fl mmproj Ar MMPROJ_FILE
Path to a multimodal projector file for LLaVA.
.It Fl Fl log-disable
Disables logging to a file.
.It Fl Fl nobrowser
Do not attempt to open a web browser tab at startup.
.It Fl Fl unsecure
Disables pledge() sandboxing on Linux and OpenBSD
.El
.Sh SEE ALSO
.Xr llamafile 1 ,
.Xr llamafile-quantize 1 ,
.Xr llamafile-perplexity 1 ,
.Xr llava-quantize 1 ,
.Xr zipalign 1 ,
.Xr unzip 1
